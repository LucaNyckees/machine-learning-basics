{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can produce a prediction file for the data of the test set for the six different methods that were required. The cross validation of the hyperparameters of the least square with normal equations and logistic regression can be done at the end of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from implementations import *\n",
    "from cleaning_data import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "DATA_TEST_PATH = 'test.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the model you will have to do the three essential following tasks: \n",
    "\n",
    "   1) Choose the type of regression: least squares or logistic regression.(To do so put a # in front of the unwanted choice and remove it in front of the wanted choice in the next cell)  \n",
    "   \n",
    "   2) Choose the parameters of the chosen method\n",
    "   \n",
    "   3) Choose the method. (To do so put a # in front of the unwanted choice and remove it in front of the wanted choice in the second next cell)\n",
    "   \n",
    "   4) Be sure to have train.csv and test.csv in the same folder as this file\n",
    "\n",
    "The run.py file and this notebook output with least_squares(y,x) and degree=9 are the same. Be sure to run the cells from the top to the bottom to produce the output file with the name OUTPUT_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of regression------------------------------------------------------------------------------------------\n",
    "\n",
    "choice='least squares'\n",
    "#choice='logistic regression'\n",
    "\n",
    "#Choice of parameters------------------------------------------------------------------------------------------\n",
    "\n",
    "degree=9                       #choice of the degree of the polynomial expansion\n",
    "max_iters=50                   #max number of iterations for GD and SGD\n",
    "gamma=0.007                    #step size for GD and SGD\n",
    "lambda_=0.001                  #factor of the regularization for the ridge and logistic regression\n",
    "OUTPUT_PATH='submission.csv'   #name of the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.826704"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and adapting the data---------------------------------------------------------------------------------\n",
    "\n",
    "x_0,x_1,x_23=adapt_x(tX,degree)\n",
    "if choice=='least squares':\n",
    "    y_0,y_1,y_23=adapt_y_least_squares(y,tX)\n",
    "elif choice=='logistic regression':\n",
    "    y_0,y_1,y_23=adapt_y_logistic(y,tX)\n",
    "else:\n",
    "    raise SyntaxWarning\n",
    "\n",
    "#Computing the weights and loss on the training set-------------------------------------------------------------\n",
    "\n",
    "#Gradient descent\n",
    "#w_0, loss_w0 = least_squares_SGD(y_0, x_0, np.zeros(x_0.shape[1]), max_iters,gamma)\n",
    "#w_1, loss_w1 = least_squares_SGD(y_1, x_1, np.zeros(x_1.shape[1]), max_iters,gamma)\n",
    "#w_23, loss_w2 = least_squares_SGD(y_23, x_23, np.zeros(x_23.shape[1]), max_iters,gamma)\n",
    "\n",
    "#Stochastic gradient descent\n",
    "#w_0, loss_w0 = least_squares_SGD(y_0, x_0, np.zeros(x_0.shape[1]), max_iters,gamma)\n",
    "#w_1, loss_w1 = least_squares_SGD(y_1, x_1, np.zeros(x_1.shape[1]), max_iters,gamma)\n",
    "#w_23, loss_w2 = least_squares_SGD(y_23, x_23, np.zeros(x_23.shape[1]), max_iters,gamma)\n",
    "\n",
    "#Least squares with normal equations\n",
    "w_0, loss_w0 = least_squares(y_0, x_0)\n",
    "w_1, loss_w1 = least_squares(y_1, x_1)\n",
    "w_23, loss_w2 = least_squares(y_23, x_23)\n",
    "\n",
    "#Ridge regression\n",
    "#w_0, loss_w0 = ridge_regression(y_0, x_0, lambda_)\n",
    "#w_1, loss_w1 = ridge_regression(y_1, x_1, lambda_)\n",
    "#w_23, loss_w2 = ridge_regression(y_23, x_23, lambda_)\n",
    "\n",
    "#Logistic regression\n",
    "#w_0, loss_w0 = logistic_regression(y_0, x_0, np.zeros(x_0.shape[1]), max_iters,gamma)\n",
    "#w_1, loss_w1 = logistic_regression(y_1, x_1, np.zeros(x_1.shape[1]), max_iters,gamma)\n",
    "#w_23, loss_w2 = logistic_regression(y_23, x_23, np.zeros(x_23.shape[1]), max_iters,gamma)\n",
    "\n",
    "#Regularized logistic regression\n",
    "#w_0, loss_w0 = reg_logistic_regression(y_0, x_0, lambda_, np.zeros(x_0.shape[1]), max_iters,gamma)\n",
    "#w_1, loss_w1 = reg_logistic_regression(y_1, x_1, lambda_, np.zeros(x_1.shape[1]), max_iters,gamma)\n",
    "#w_23, loss_w2 = reg_logistic_regression(y_23, x_23, lambda_, np.zeros(x_23.shape[1]), max_iters,gamma)\n",
    "\n",
    "#Predict on the train set----------------------------------------------------------------------------------------\n",
    "\n",
    "y_pred_tr = label(w_0,w_1,w_23,x_0,x_1,x_23,tX,choice)\n",
    "compute_accuracy(y, y_pred_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and adapting the data----------------------------------------------------------------------------------\n",
    "x_0_te,x_1_te,x_23_te=adapt_x(tX_test,degree)\n",
    "\n",
    "#Predict on the test set-----------------------------------------------------------------------------------------\n",
    "y_pred_te=label(w_0,w_1,w_23,x_0_te,x_1_te,x_23_te,tX_test,choice)\n",
    "create_csv_submission(ids_test, y_pred_te, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation of the hyperparameters has been explicitely coded only for the least square with normal equations and the logistic regression. However the code can be easily adapted to the others method only by changing the name of the method in the cross_validation.py file. For one parameter validation, please modify the function cross_val_LS and for two parameters validation please modify the function cross_val_Log.\n",
    "\n",
    "Every cross validation will be done with 5 folds.\n",
    "\n",
    "The cross_val_LS takes the raw data and the maximum degree of the polynomial extension you want to test as input. The output is a boxplot of the accuracy on each fold for each degree from 1 to max_degree.\n",
    "\n",
    "The cross_val_Log takes the raw data, the maximum degree of the polynomial extension you want to test and the different gamma you want to test (as an array) as input. The output  is a max_degree\\*len(gamma)\\*2 array where the element [d,i,0] is the mean of the accuracy on the k folds for degree d and gamma gamma[i] and [d,i,1] is the std of the accuracy on the k folds for degree d and gamma[i]. Keep in mind that this method is quite slow and that you should consider relatively small gamma (below 10^-2) to avoid computation problem with the sigmoid function.\n",
    "\n",
    "In both case, the choice of the hyperparameters has to be done with a grid search on the output or by manually check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation for the least square method\n",
    "max_degree=12\n",
    "k_fold=5\n",
    "cross_val_LS(y,tX,k_fold,max_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphaelmirallie/Documents/EPFL/aMA_1/Machine_learning/ML_Project1/Projet/scripts/helper_implementations.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  loss=y.T.dot(np.log(pred)) + (1-y).T.dot(np.log(1-pred))\n",
      "/Users/raphaelmirallie/Documents/EPFL/aMA_1/Machine_learning/ML_Project1/Projet/scripts/implementations.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < treshold:\n",
      "/Users/raphaelmirallie/Documents/EPFL/aMA_1/Machine_learning/ML_Project1/Projet/scripts/helper_implementations.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1+np.exp(-t))\n",
      "/Users/raphaelmirallie/Documents/EPFL/aMA_1/Machine_learning/ML_Project1/Projet/scripts/helper_implementations.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss=y.T.dot(np.log(pred)) + (1-y).T.dot(np.log(1-pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6.57332000e-01 4.28177533e-04]\n",
      "  [6.57332000e-01 4.28177533e-04]\n",
      "  [6.57332000e-01 4.28177533e-04]\n",
      "  [6.57324000e-01 4.27731224e-04]\n",
      "  [6.66099000e-01 3.34356098e-04]\n",
      "  [7.06400000e-01 3.92721275e-04]\n",
      "  [7.24076000e-01 3.58725522e-04]\n",
      "  [7.33398000e-01 2.14676501e-04]\n",
      "  [7.35289000e-01 2.73667682e-04]\n",
      "  [7.35760000e-01 2.80124972e-04]\n",
      "  [7.35833000e-01 2.71488490e-04]\n",
      "  [7.35836000e-01 2.69951848e-04]]\n",
      "\n",
      " [[6.57332000e-01 4.28177533e-04]\n",
      "  [6.57332000e-01 4.28177533e-04]\n",
      "  [6.57332000e-01 4.28177533e-04]\n",
      "  [6.57332000e-01 4.28177533e-04]\n",
      "  [6.57827000e-01 4.24824670e-04]\n",
      "  [6.62958000e-01 3.67336358e-04]\n",
      "  [6.66417000e-01 3.91415380e-04]\n",
      "  [6.67093000e-01 3.90788946e-04]\n",
      "  [6.67168000e-01 3.68274354e-04]\n",
      "  [6.67177000e-01 3.69927020e-04]\n",
      "  [6.67183000e-01 3.67023160e-04]\n",
      "  [6.67184000e-01 3.65162977e-04]]\n",
      "\n",
      " [[6.57336000e-01 4.27661081e-04]\n",
      "  [6.57474000e-01 4.22248742e-04]\n",
      "  [6.59428000e-01 4.04482385e-04]\n",
      "  [6.62768000e-01 3.79586617e-04]\n",
      "  [6.53567000e-01 1.52435101e-03]\n",
      "  [6.44877000e-01 2.15795876e-03]\n",
      "  [6.43582000e-01 2.44286430e-03]\n",
      "  [6.43544000e-01 2.51018406e-03]\n",
      "  [6.43544000e-01 2.49737342e-03]\n",
      "  [6.43547000e-01 2.49534086e-03]\n",
      "  [6.43553000e-01 2.50092303e-03]\n",
      "  [6.43551000e-01 2.49800200e-03]]]\n"
     ]
    }
   ],
   "source": [
    "#Cross validation for the logistic regression method\n",
    "#Second warning: this method is slow and can take a lot of time\n",
    "max_degree=3\n",
    "k_fold=5\n",
    "gamma=np.logspace(-6,0,6)\n",
    "print(cross_val_Log(y,tX,k_fold,max_degree,gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
